{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are great in that they are easy to make and easy to interpret. But a _serious_ flaw of theirs is that they have a lot of variance b/w each tree iteration and can easily overfit. A way to combat this is to have more than one decision tree. We can make as many trees as we like, a whole \"forest\" even. Then each tree is given a vote which is aggregated in the final decision of the outcome. This __Random Forest__ can be used as a classifier or in a regression. As a classifier, aggregation is based on the most popular outcome from the trees. In a regression, the aggregation is typically the average of the mean of each tree's prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params\n",
    "When building a random forest, we can set the parameters of the trees and forest itself. The trees have the same parameters: max features, random state, max depth of branches, and building the tree using information gain and entropy or some other method like [Gini impurity](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria).<br>\n",
    "You can also control the number of estimators, or decision trees, in the forest. Deciding this is a blancing game between how much variance you can explain and computational complexity. As we add more trees to the forest, the information added converges to a point. That's because there's only so much information to be learned, and once an acceptable variance in accuracy is reached we can use that as a cutoff for the number of trees generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Random subspace\n",
    "Random forests don't just create trees from the same data over and over again. This would result in high correlations between estimators which can lead to bias as highly predictive features begin to dominate the trees. This lead to many trees with potentially the same/similar information, which can lead to very similar, potentially biased predictions. <br>\n",
    "We fix this by applying __bagging__. Each tree picks an observation with replacement to build the training set. This means that the same observation can show up in multiple trees. This is only really a problem if the number of observations is very low. Also, random forests only use a random subset of features for deciging splits/rules. This means that for each rule we make, we are only looking at the __random subspace__ created by a random subset of _some_ of the features as possibilities to generate the rule. This helps to avoid the correlation problem of having the same features be used for each split. As a general rule for a dataset with x features, classifiers use $\\sqrt{x}$ features and regression use $x/3$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages\n",
    "### Pros\n",
    "* Very flexible, requires no input preparation\n",
    "* They can handle most/all data types\n",
    "* Very quick to train and have low variance with high accuracy (Good for using as a benchmark model)\n",
    "\n",
    "### Cons\n",
    "* Showcases the biggest weakness of supervised learning, it can only predict what it has already seen.\n",
    "* Can get very large, computationally expensive, and complex if it grows too deep\n",
    "* Very much a __black box__ model in that an output is given with very little insight as to how we got there\n",
    "* You dont get much insight into the process of prediction and which features matter, so you can't visually represent the process or learn about the underlying process\n",
    "\n",
    "Overall, this model works well in the beginning and we can validate the results later on as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Intro to Ensemble Models\n",
    "At its core, it's a model that is made up of other models. The component models are usually simple ones, but you can technically ocmbine most models together to form an ensemble.\n",
    "\n",
    "### Ways to assemble an ensemble model\n",
    "* __Bagging__: You take subsets of the data and train a model on each subset, which then they vote together to give an outcome, based either on the majority (Classifiers) or the mean of the results (Regressions). ex: Random Forest\n",
    "* __Boosting__: Uses the output from one model as input for the next in a form of serial processing. These chains keep happening until a stop condition is reached.\n",
    "* __Stacking__: 2 phase process. 1. Multiple models are trained in parallel. 2. The output from those models is used as inout for the final model to give a prediction. Essentially, it combines the parallel approach of bagging witht the serialization of boosting forming a hybrid model.\n",
    "\n",
    "Ensemble models are usually used for their high accuracy and low variance because they're built from internal models. However, techniques such as boosting are prone to overfitting. You also lose a lot of transparency from single models, as from the example about random forests. Decision trees are easy to extract information from but turn it into a randomm forest and we lose a lot of the internal information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohai\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (0,19,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Replace the path with the correct path for your data.\n",
    "y2015 = pd.read_csv(\n",
    "    'https://www.dropbox.com/s/0so14yudedjmm5m/LoanStats3d.csv?dl=1',\n",
    "    skipinitialspace=True,\n",
    "    header=1\n",
    ")\n",
    "# Note the warning about dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "421097\n",
      "term\n",
      "2\n",
      "int_rate\n",
      "110\n",
      "grade\n",
      "7\n",
      "sub_grade\n",
      "35\n",
      "emp_title\n",
      "120812\n",
      "emp_length\n",
      "11\n",
      "home_ownership\n",
      "4\n",
      "verification_status\n",
      "3\n",
      "issue_d\n",
      "12\n",
      "loan_status\n",
      "7\n",
      "pymnt_plan\n",
      "1\n",
      "url\n",
      "421095\n",
      "desc\n",
      "34\n",
      "purpose\n",
      "14\n",
      "title\n",
      "27\n",
      "zip_code\n",
      "914\n",
      "addr_state\n",
      "49\n",
      "earliest_cr_line\n",
      "668\n",
      "revol_util\n",
      "1211\n",
      "initial_list_status\n",
      "2\n",
      "last_pymnt_d\n",
      "25\n",
      "next_pymnt_d\n",
      "4\n",
      "last_credit_pull_d\n",
      "26\n",
      "application_type\n",
      "2\n",
      "verification_status_joint\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "categorical = y2015.select_dtypes(include=['object'])\n",
    "for i in categorical:\n",
    "    column = categorical[i]\n",
    "    print(i)\n",
    "    print(column.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert ID and Interest Rate to numeric.\n",
    "y2015['id'] = pd.to_numeric(y2015['id'], errors='coerce')\n",
    "y2015['int_rate'] = pd.to_numeric(y2015['int_rate'].str.strip('%'), errors='coerce')\n",
    "\n",
    "# Drop other columns with many unique variables\n",
    "y2015.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['last_pymnt_d' 'next_pymnt_d' 'title' 'issue_d' 'last_pymnt_amnt'\\n 'last_credit_pull_d' 'member_id' 'id' 'emp_length'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2c3b05706547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m y2015.drop(\n\u001b[0;32m      4\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'last_pymnt_d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'next_pymnt_d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'issue_d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'last_pymnt_amnt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'last_credit_pull_d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'member_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'emp_length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m axis=1, inplace=True)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['last_pymnt_d' 'next_pymnt_d' 'title' 'issue_d' 'last_pymnt_amnt'\\n 'last_credit_pull_d' 'member_id' 'id' 'emp_length'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#variables related to time, but have the tendency to contribute little information\n",
    "#Too many dummy variables created\n",
    "y2015.drop(\n",
    "['last_pymnt_d', 'next_pymnt_d', 'title', 'issue_d', 'last_pymnt_amnt', 'last_credit_pull_d', 'member_id', 'id', 'emp_length'],\n",
    "axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate',\n",
       "       'installment', 'grade', 'home_ownership', 'annual_inc',\n",
       "       'verification_status', 'loan_status', 'pymnt_plan', 'purpose', 'dti',\n",
       "       'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq',\n",
       "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
       "       'total_acc', 'initial_list_status', 'out_prncp', 'out_prncp_inv',\n",
       "       'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',\n",
       "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
       "       'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
       "       'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint',\n",
       "       'verification_status_joint', 'acc_now_delinq', 'tot_coll_amt',\n",
       "       'tot_cur_bal', 'open_acc_6m', 'open_il_6m', 'open_il_12m',\n",
       "       'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util',\n",
       "       'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util',\n",
       "       'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
       "       'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
       "       'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
       "       'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\n",
       "       'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
       "       'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n",
       "       'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
       "       'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
       "       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m',\n",
       "       'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
       "       'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies',\n",
       "       'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit',\n",
       "       'total_il_high_credit_limit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y2015.columns))\n",
    "y2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2015 = y2015[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95089169, 0.96426112, 0.96255135, 0.96115029, 0.96053194,\n",
       "       0.96060318, 0.960531  , 0.96048351, 0.96038663, 0.9602432 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "X = y2015.drop(['loan_status'], 1)\n",
    "Y = y2015['loan_status']\n",
    "X = pd.get_dummies(X)\n",
    "#There are 400,000+ rows so its ok if we don't impute the columns for missingness. We can just drop them and still have\n",
    "#rich enough information\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loan_amnt', 0.009550317979231799)\n",
      "('funded_amnt', 0.055085643109397356)\n",
      "('open_acc', 0.243768274900885)\n",
      "('pub_rec', 0.306878827921759)\n",
      "('revol_bal', 0.07031539328243734)\n",
      "('total_acc', 0.049392207749760136)\n",
      "('out_prncp', 0.07419500147437742)\n",
      "('out_prncp_inv', 0.03119600501887909)\n",
      "('total_pymnt_inv', 0.024704989832801626)\n",
      "('total_rec_prncp', 0.015222540801252857)\n"
     ]
    }
   ],
   "source": [
    "feat_labels = X.columns\n",
    "clean_col = []\n",
    "for feature in zip(feat_labels, rfc.feature_importances_):\n",
    "    if (feature[1]>.009):\n",
    "        print(feature)\n",
    "        clean_col.append(feature[0])\n",
    "remove_loan = ['loan_amnt', 'funded_amnt_inv', 'funded_amnt']\n",
    "clean_col = [x for x in clean_col if x not in remove_loan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['open_acc', 'pub_rec', 'revol_bal', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt_inv', 'total_rec_prncp']\n"
     ]
    }
   ],
   "source": [
    "#No loan_amnt', 'funded_amnt_inv', 'funded_amnt'\n",
    "print(clean_col)\n",
    "X_new = y2015.loc[:,clean_col]\n",
    "X_new.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89985989, 0.96606587, 0.9625751 , 0.95846691, 0.95746853,\n",
       "       0.95732605, 0.95523522, 0.9560664 , 0.95658679, 0.95537453])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rfc, X_new, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('open_acc', 0.009550317979231799)\n",
      "('pub_rec', 0.055085643109397356)\n",
      "('revol_bal', 0.005412974573836585)\n",
      "('total_acc', 0.005674197414755255)\n",
      "('total_rec_prncp', 0.007994133185355605)\n"
     ]
    }
   ],
   "source": [
    "feat_labels = X_new.columns\n",
    "v3_col = []\n",
    "for feature in zip(feat_labels, rfc.feature_importances_):\n",
    "    if (feature[1]>.005):\n",
    "        print(feature)\n",
    "        v3_col.append(feature[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v3 = y2015.loc[:, v3_col]\n",
    "X_v3.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421095"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v3 = pd.concat([X_v3, y2015['out_prncp']], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93381777, 0.95207903, 0.95055924, 0.94894446, 0.94735217,\n",
       "       0.94533365, 0.94438244, 0.94393123, 0.94380982, 0.93817983])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rfc, X_v3, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
